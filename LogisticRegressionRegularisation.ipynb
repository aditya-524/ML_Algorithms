{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularising Logistic Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation if regularized logistic regression. \n",
    "\n",
    "The regularized cost function in logistic regression is\n",
    "\n",
    "$$ J(W) = \\frac{1}{n} \\left[ -Y \\log\\left(h\\left( X \\right) \\right) - \\left( 1 - Y\\right) \\log \\left( 1 - h\\left( X \\right) \\right) \\right] + \\frac{\\lambda}{2n} \\sum_{j=1}^m w_j^2 \\qquad \\text{ for } j > 0$$\n",
    "\n",
    "weshould not regularize the parameters $w_0$. The gradient of the cost function is a vector defined as follows:\n",
    "\n",
    "$$ \\frac{\\partial J(W)}{\\partial w_0} =\\nabla_{w_0} = \\frac{1}{n} \\left( h \\left( X \\right) - Y \\right) X_0 \\qquad \\text{ where } X_0 \\text{ is a vector or 1's, corresponding to intercept } w_0 $$\n",
    "\n",
    "$$ \\frac{\\partial J(W)}{\\partial w_j} =\\nabla_{w_j} = \\frac{1}{n} \\left( h \\left( X \\right) - Y \\right) X_j  + \\frac{\\lambda}{m}w_j \\qquad \\text{ where } X_j \\\\ \\text{ is the array of X's, except } X_0 \n",
    "\\text{ corresponding to intercept } w_0 $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# DO NOT use any other import statements for this question\n",
    "\n",
    "df = pd.read_csv('titanic.csv')\n",
    "data = df[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch']].dropna()\n",
    "data.loc[data[\"Sex\"] == \"male\", \"Sex\"] = 1\n",
    "data.loc[data[\"Sex\"] == \"female\", \"Sex\"] = 0\n",
    "data = np.array(data)\n",
    "X, Y = data[:, 1:], data[:, 0]\n",
    "\n",
    "# normalise all cols\n",
    "for c in range(X.shape[1]):\n",
    "    X[:,c] = (max(X[:,c]) -  X[:,c])/(max(X[:,c]) - min(X[:,c]))\n",
    "    \n",
    "# break into train/test\n",
    "split = int(0.8 * data.shape[0])\n",
    "\n",
    "X_train = X[:split]\n",
    "X_test = X[split:]\n",
    "Y_train = Y[:split]\n",
    "Y_test = Y[split:]\n",
    "\n",
    "# Add intercept term to X\n",
    "X_train = np.concatenate([np.ones((X_train.shape[0], 1)), X_train], axis=1)\n",
    "X_test = np.concatenate([np.ones((X_test.shape[0], 1)), X_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute sigmoid function given the input z.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    z : array_like\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    g : array_like\n",
    "        The computed sigmoid function. g has the same shape as z, since\n",
    "        the sigmoid is computed element-wise on z.\n",
    "        \n",
    "    Instructions\n",
    "    ------------\n",
    "    Computing the sigmoid of each value of z (z can be a matrix, vector or scalar).\n",
    "    \"\"\"\n",
    "    # converting input to a numpy array\n",
    "    z = np.array(z).astype(\"float\")\n",
    "        \n",
    "    # ====================== CODE HERE ======================  \n",
    "    g = 1 / (1 + np.exp(-z)) # This is the basic formula for sigmoid\n",
    "    # =============================================================\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_costFunctionReg(W, X, Y, lambda_):\n",
    "    \"\"\"\n",
    "    Compute cost and gradient for logistic regression with regularization.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    W : Logistic regression vector of m parameters,\n",
    "        where m is the number of features including any intercept.\n",
    "    \n",
    "    X : The data set with shape (n,m). n is the number of examples, and\n",
    "        m is the number of features.\n",
    "    \n",
    "    y : The vector of data labels of size n.\n",
    "    \n",
    "    lambda_ : float\n",
    "        The regularization parameter. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    J : float\n",
    "        The computed value for the regularized cost function. \n",
    "   \n",
    "    \"\"\"\n",
    "    # Initialize some useful values\n",
    "    n = Y.size  # number of training examples\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    J = 0\n",
    "    # ===================== CODE HERE ======================\n",
    "    W_temp = np.copy(W)\n",
    "    W_temp[0] = 0 # To make the wj term as per the equation, to multiply with regularising term\n",
    "    h = sigmoid(np.dot(X, W))\n",
    "    J = (-1/n) * (np.dot(Y, np.log(h)) + np.dot((1-Y) , np.log(1-h))) + (lambda_/(2*n)) * np.sum(np.square(W_temp))\n",
    "    \n",
    "    return J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_GradFunctionReg(W, X, Y, lambda_):\n",
    "    \"\"\"\n",
    "    Compute cost and gradient for logistic regression with regularization.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    W : Logistic regression vector of m parameters,\n",
    "        where m is the number of features including any intercept.\n",
    "    \n",
    "    X : The data set with shape (n,m). n is the number of examples, and\n",
    "        m is the number of features.\n",
    "    \n",
    "    y : The vector of data labels of size n.\n",
    "    \n",
    "    lambda_ : float\n",
    "        The regularization parameter. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    grad : A vector of size m which is the gradient of the cost\n",
    "        function with respect to theta, at the current values of theta.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Initializing some useful values\n",
    "    n = Y.size  # number of training examples\n",
    "\n",
    "    # To return the following variables correctly \n",
    "    grad = np.zeros(W.shape)\n",
    "\n",
    "    # ===================== CODE HERE ======================\n",
    "    h = sigmoid(np.dot(X, W))\n",
    "    W_temp = np.copy(W)\n",
    "    W_temp[0] = 0\n",
    "    grad = (1 / n) * (np.dot((h - Y) , X))\n",
    "    grad = grad + (lambda_ / n) * W_temp\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_gradient_descent_reg(X, Y, W, cost_function, gradient_function, alpha, num_iters, lambda_): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X                   : Data ndarray array, n examples with m features\n",
    "      Y                   : ndarray vector of target n values\n",
    "      W                   : ndarray vector of initial m model parameters \n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      W (ndarray)         : Updated values of parameters \n",
    "      \"\"\"\n",
    "    \n",
    "    # =====================CODE HERE ======================\n",
    "    grad = np.zeros(W.shape)\n",
    "    for i in range(num_iters):\n",
    "        grad = gradient_function(W, X, Y, lambda_)\n",
    "        W = W -  alpha * grad\n",
    "        J = cost_function(W, X, Y, lambda_)\n",
    "    # =============================================================\n",
    "    \n",
    "    return J, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please copy the folowing result line to Question 4 \"(sumW = )\"\n",
      "1.95\n",
      "Please copy the folowing result line to Question 4 \"(J = )\"\n",
      "0.49\n",
      "Please copy the folowing result line to Question 4 \"(Accuracy = )\"\n",
      "0.8\n"
     ]
    }
   ],
   "source": [
    "# initialize parameters\n",
    "initial_W = np.array([-40]*X_train.shape[1])\n",
    "\n",
    "# some gradient descent settings\n",
    "iterations = 20000\n",
    "alpha = 0.02\n",
    "lambda_ = 2\n",
    "W = 0\n",
    "J = 0\n",
    "acc = 0\n",
    "\n",
    "\"\"\"\n",
    "Apply functions coded above to calculate:\n",
    "    final W after training,\n",
    "    cost J for training set after training\n",
    "    accuracy for test set\n",
    "Using given datasets and parameters\n",
    "\"\"\"\n",
    "\n",
    "# ===================== CODE HERE ======================\n",
    "J, W = logreg_gradient_descent_reg(X_train, Y_train, initial_W, logreg_costFunctionReg, logreg_GradFunctionReg, alpha, iterations, lambda_)\n",
    "y_pred = np.round(sigmoid(np.dot(X_test, W)))\n",
    "correct = np.sum(y_pred == Y_test)\n",
    "acc = correct / len(Y_test)\n",
    "\n",
    "# ===========================================================\n",
    "\n",
    "print('Please copy the folowing result line to Question 4 \"(sumW = )\"')\n",
    "print(np.round(np.sum(W), 2))\n",
    "print('Please copy the folowing result line to Question 4 \"(J = )\"')\n",
    "print(np.round(J,2))\n",
    "print('Please copy the folowing result line to Question 4 \"(Accuracy = )\"')\n",
    "print(np.round(acc,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
