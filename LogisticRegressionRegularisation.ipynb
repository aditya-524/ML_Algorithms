{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Gradient Descent, Python Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the code where indicated implementing logistic legression and gradient descent. Recall that the cost function in logistic regression is\n",
    "\n",
    "$$ J(W) = \\frac{1}{n} \\left[ -Y \\log\\left(h\\left( X \\right) \\right) - \\left( 1 - Y\\right) \\log \\left( 1 - h\\left( X \\right) \\right) \\right]$$\n",
    "\n",
    "and the gradient of the cost is a vector of the same length as $W$ defined as follows:\n",
    "\n",
    "$$ \\frac{\\partial J(W)}{\\partial W} =\\nabla_W = \\frac{1}{n} \\left( h \\left( X \\right) - Y \\right) X $$\n",
    "\n",
    "Note that while this gradient looks identical to the linear regression gradient, the formula is actually different because linear and logistic regression have different definitions of hypothesis function $h(X)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('titanic.csv')\n",
    "data = df[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch']].dropna()\n",
    "data.loc[data[\"Sex\"] == \"male\", \"Sex\"] = 1\n",
    "data.loc[data[\"Sex\"] == \"female\", \"Sex\"] = 0\n",
    "data = np.array(data)\n",
    "X, Y = data[:, 1:], data[:, 0]\n",
    "\n",
    "# normalise all columns of X using min-max\n",
    "for c in range(X.shape[1]):\n",
    "    X[:,c] = (max(X[:,c]) -  X[:,c])/(max(X[:,c]) - min(X[:,c]))\n",
    "    \n",
    "# break into train/test, with 80% training and 20% test\n",
    "split = int(0.8 * data.shape[0])\n",
    "\n",
    "X_train = X[:split]\n",
    "X_test = X[split:]\n",
    "Y_train = Y[:split]\n",
    "Y_test = Y[split:]\n",
    "\n",
    "# Add intercept term to X_train and X_test\n",
    "X_train = np.concatenate([np.ones((X_train.shape[0], 1)), X_train], axis=1)\n",
    "X_test = np.concatenate([np.ones((X_test.shape[0], 1)), X_test], axis=1)\n",
    "\n",
    "# ================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute sigmoid function given the input z.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    z : array_like\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    g : array_like\n",
    "        The computed sigmoid function. g has the same shape as z, since\n",
    "        the sigmoid is computed element-wise on z.\n",
    "        \n",
    "    Instructions\n",
    "    ------------\n",
    "    Computing the sigmoid of each value of z (z can be a matrix, vector or scalar).\n",
    "    \"\"\"\n",
    "    # converting input to a numpy array\n",
    "    z = np.array(z).astype(\"float\")\n",
    "        \n",
    "    # ====================== CODE HERE ======================  \n",
    "    g = 1 / (1 + np.exp(-z)) # This is the basic formula for sigmoid\n",
    "    # =============================================================\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_costFunction(W, X, Y):\n",
    "    \"\"\"\n",
    "    Compute cost and gradient for logistic regression. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    W : m parameters for logistic regression. \n",
    "    \n",
    "    X : The input dataset of shape (n,m) where n is the total number\n",
    "        of data points and m is the number of features. We assume the \n",
    "        intercept has already been added to the input.\n",
    "    \n",
    "    Y : Vector of labels for the input with n elements. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    J : float\n",
    "        The computed value for the cost function. \n",
    "    \n",
    "        \n",
    "    \"\"\"\n",
    "    # Initialize some useful values\n",
    "    n = Y.size  # number of training examples\n",
    "    J = 0\n",
    "    \n",
    "    # ====================== CODE HERE ======================\n",
    "    # NOTE: the diff between dot and @ is that dot allow nultiply matrix by scalar, @ does not\n",
    "    #This is the Elementwise operation(s)\n",
    "    for i in range(n): #Used element wise calculation instead of Matrix Multiplication\n",
    "        z = np.dot(X[i] , W)\n",
    "        h_x = sigmoid(z) # Since h(x) is a sigmoid functiton\n",
    "        J +=  - Y[i] *np.log(h_x) - (1 - Y[i]) * np.log(1 - h_x) # As per the equation given in the question \n",
    "    J = J / n\n",
    "\n",
    "    #Alternative attempt for Matrix Based calculation, which i did not continue due to ease of understanding of Elementwise\n",
    "    #Though, I did find the elementwise operation to be costly for compute and takes 4+ minutes to calculates.\n",
    "    # h = sigmoid(np.dot(X, W))\n",
    "    #J = -(1 / n) * (Y.dot(np.log(h)) + (1 - Y).dot(np.log(1 - h)))\n",
    "    # =============================================================\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_GradFunction(W, X, Y):\n",
    "    \"\"\"\n",
    "    Compute gradient for logistic regression. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    W : m parameters for logistic regression. \n",
    "    \n",
    "    X : The input dataset of shape (n,m) where n is the total number\n",
    "        of data points and m is the number of features. We assume the \n",
    "        intercept has already been added to the input.\n",
    "    \n",
    "    Y : Vector of labels for the input with n elements. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    grad : A vector with m values which is the gradient of the cost\n",
    "        function with respect to W, at the current values of W.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Initialize some useful values\n",
    "    n = Y.size  # number of training examples\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    grad = np.zeros(W.shape)\n",
    "    \n",
    "    # ====================== CODE HERE ======================\n",
    "    # NOTE: the diff between dot and @ is that dot allow nultiply matrix by scalar, @ does not\n",
    "    #The equation is very basic conversion from the given equation above the question, still implementing elementwise \n",
    "    #Similar to above, I am using Elementiwise operation for ease of understanding while sacrificing compute and time\n",
    "    m = W.shape[0]\n",
    "    for i in range(n):\n",
    "        z = np.dot(X[i] , W)\n",
    "        h_x = sigmoid(z)\n",
    "        for j in range(m):\n",
    "            grad[j] += (h_x- Y[i]) * X[i,j]\n",
    "    grad = grad / n\n",
    "\n",
    "    #Matrix Wise Operation alternative, short and simple\n",
    "    # h = sigmoid(np.dot(X, W))\n",
    "    # grad = 1/n * np.dot(X.T, (h - Y))\n",
    "    # =============================================================\n",
    "    # print(f\" H_X Size : {h_x.shape}\")\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_gradient_descent(X, Y, W_in, cost_function, gradient_function, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn W. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X                   : Data, n examples with m features\n",
    "      Y                   : m target values\n",
    "      W_in                : m initial model parameters  \n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      W                : Updated values of parameters \n",
    "      \"\"\"\n",
    "    W = W_in\n",
    "    \n",
    "    # ====================== CODE HERE ======================\n",
    "\n",
    "    for i in range(num_iters):\n",
    "      grad = gradient_function(W, X, Y)\n",
    "      W = W -  alpha * grad\n",
    "      J = cost_function(W, X, Y) \n",
    "          \n",
    "    # Add the cost function to the array\n",
    "    # =============================================================\n",
    "      \n",
    "    return J, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please copy the folowing result line to Question 3 \"(sumW = )\"\n",
      "0.4\n",
      "Please copy the folowing result line to Question 3 \"(J = )\"\n",
      "0.56\n",
      "Please copy the folowing result line to Question 3 \"(Accuracy = )\"\n",
      "0.78\n"
     ]
    }
   ],
   "source": [
    "# initialize parameters\n",
    "initial_W = np.array([-40.0]*X_train.shape[1])\n",
    "\n",
    "# some gradient descent settings\n",
    "iterations = 20000\n",
    "alpha = 0.02\n",
    "\n",
    "\"\"\"\n",
    "Apply functions coded above to calculate final W, cost J and accuracy of predictions\n",
    "Use given datasets and parameters\n",
    "\"\"\"\n",
    "J = 0\n",
    "W = 0 \n",
    "acc = 0\n",
    "\n",
    "# ====================== CODE HERE ======================\n",
    "# NOTE: to print correctly, W must be of shape (5,), J must be scalar float\n",
    "J, W = logreg_gradient_descent(X_train, Y_train, initial_W, logreg_costFunction, logreg_GradFunction, alpha, iterations)\n",
    "y_pred = np.round(sigmoid(np.dot(X_test, W)))\n",
    "correct = np.sum(y_pred == Y_test)\n",
    "acc = correct / len(Y_test)\n",
    "# ===========================================================\n",
    "\n",
    "print('Please copy the folowing result line to Question 3 \"(sumW = )\"')\n",
    "print(np.round(np.sum(W), 2)) \n",
    "print('Please copy the folowing result line to Question 3 \"(J = )\"')\n",
    "print(np.round(J,2))\n",
    "print('Please copy the folowing result line to Question 3 \"(Accuracy = )\"')\n",
    "print(np.round(acc,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
