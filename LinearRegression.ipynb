{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Derivation and Python Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression can be expressed as $y_k=w_0\\times 1 + w_1x_{n1}+...+w_{m-1}x_{k,m-1}$, <br>\n",
    "where the training dataset $( {X}, {Y})$ has $n$ examples and $m$ features (where the first feature is 1).\n",
    "Equivalent matrix form is $\\hat{ {Y}}= {X} {W}$, where $X$ is a $n \\times m$ matrix, and $ {W}$ is a $m \\times 1$ vector of parameters and $\\hat{ {Y}}$ is the vector of predicted values y. Note that the first element of $W$ which is $w_0$ in an intercept.\n",
    "\n",
    "The cost function is $$J( {W}) = \\frac{1}{2n} \\sum\\limits_{i=0}^n(\\hat{y_i} - y_i)^2 \\tag{1}$$\n",
    "In the matrix form the cost function is $$J( {W})=\\frac{1}{2n}||\\hat{ {Y}} -  {Y}||_2^2=\\frac{1}{2n}|| {X} {W}- {Y}||_2^2 = \\frac{1}{2n}( {X} {W}- {Y})^T( {X} {W}- {Y})$$\n",
    "\n",
    "In order to find $ {W}$, we need to calculate the derivative of $J$ with respect to $ {W}$: $$\\frac{\\partial{J( {W})}}{\\partial{ {W}}}=\\nabla_w(J( {W}))$$ and compare it to zero. \n",
    "\n",
    "\n",
    "In order to find $ {W}$, we need to calculate the derivative of $J$ with respect to $ {W}$: $$\\frac{\\partial{J( {W})}}{\\partial{ {W}}}=\\nabla_w(J( {W}))$$ and compare it to zero. \n",
    "\n",
    "$$\\frac{\\partial{J( {W})}}{\\partial{ {W}}}=\\nabla_w(J( {W}))$$\n",
    "$$J( {W}) = \\frac{1}{2n}( {X} {W}- {Y})^T( {X} {W}- {Y})$$\n",
    "$$\\frac{\\partial{J( {W})}}{\\partial{ {W}}} = \\frac{1}{2n}\\frac{\\partial{(( {X} {W}- {Y})^T( {X} {W}- {Y}))}}{\\partial{ {W}}}$$\n",
    "$$ \\frac{1}{2n}\\frac{\\partial{( {X^T} {W^T}{W}{X} - {Y^T}{X}{W} - {W^T}{X^T}{Y} + {Y^T}{Y})}}{\\partial{ {W}}}$$\n",
    "\n",
    "Using $$ W^TX^TY=(W^TX^TY)^T=Y^TXW $$, if $W^TX^TY$ evaluates to a scalar.\n",
    "\n",
    "$$ \\frac{1}{2n}\\frac{\\partial{(  {X^T} {W^T}{W}{X} - 2 {W^T}{X^T}{Y} + {Y^T}{Y})}}{\\partial{ {W}}}$$\n",
    "\n",
    "\n",
    "1. $$ \\frac{\\partial{(W^TXX^TW)}}{\\partial{ {W}}}=2X^TXW $$  \n",
    "2. $$ \\frac{\\partial{(2W^TX^TY)}}{\\partial{ {W}}}=2X^TY $$ \n",
    "3. $$ \\frac{\\partial{(YY^T)}}{\\partial{ {W}}}=0 $$ \n",
    "\n",
    "Using equation 1, 2, and 3  we get\n",
    "\n",
    "$$ \\frac{1}{2n}( 2{X^T}{X} {W} - 2 {X^T}{Y} ) $$\n",
    "Comparing the equation to 0, we get\n",
    "$$ \\frac{1}{2n}( 2{X^T}{X} {W} - 2 {X^T}{Y} )  = 0 $$\n",
    "\n",
    "$$ {X^T}{X} {W} = {X^T}{Y} $$\n",
    "\n",
    "Multiplying both sides with $ (XX^T)^{-1} $, we get\n",
    "\n",
    "$$ {(X^TX)}^{-1}{X^T}{X}{W} = (X^TX)^{-1}{X^T}{Y} $$\n",
    "$$ W = ({X^TX})^{-1}{X^T}{Y} $$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closed Form of Linear Regression coding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please copy the folowing result to Question 1 \"(sumW = )\"\n",
      "8.21\n",
      "Please copy the folowing result to Question 1 \"(J = )\"\n",
      "3.7\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "Y = np.array([13, 8, 11, 2, 6])\n",
    "X = np.array([[1, 3, 5],[1, 6, 7],[1, 7, 8],[1, 8, 9],[1, 11, 12]])\n",
    "W = np.array([0, 0, 0])\n",
    "J = 0.0\n",
    "\n",
    "\"\"\"\n",
    "Using above given Y, X and W to calculate final W and cost J \n",
    "using the above  written derivation\n",
    "\"\"\"\n",
    "# ====================== CODE HERE ======================  \n",
    "n = len(Y) # or 4 in this case\n",
    "X_T = np.transpose(X) # for X transpose\n",
    "W =  np.linalg.inv(X_T @ X) @ X_T @ Y # using the formula that derived above, and using @ for matrix multiplication\n",
    "J = (1/(2*n)) * np.linalg.norm(X @ W - Y) ** 2 #using the formula that is given in the question for calculation of cost function \n",
    "#============================================================\n",
    "\n",
    "print('Please copy the folowing result to Question 1 \"(sumW = )\"')\n",
    "print(np.round(np.sum(W),2))\n",
    "print('Please copy the folowing result to Question 1 \"(J = )\"')\n",
    "print(np.round(J,2))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(\"diabetes.csv\")\n",
    "# selecting features\n",
    "X_train = np.array(dataset[[\"age\", \"sex\", \"bmi\", \"bp\"]])\n",
    "\n",
    "# normalising numerical features\n",
    "X_train = (np.max(X_train,axis=0)-X_train)/(np.max(X_train,axis=0)-np.min(X_train,axis=0))\n",
    "\n",
    "# adding '1' column for the intercept\n",
    "X_train = np.concatenate((np.ones(X_train.shape[0]).reshape(X_train.shape[0],1), X_train), axis=1)\n",
    "\n",
    "# forming target\n",
    "Y_train = np.array([dataset[\"target\"]])\n",
    "Y_train = Y_train.reshape(X_train.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg_compute_cost(X, Y, W): \n",
    "    \"\"\"\n",
    "    Args:\n",
    "      X (ndarray (n,m)): Data, n examples with m features\n",
    "      y (ndarray (n,1)) : target values\n",
    "      w (ndarray (m,1)) : model parameters  \n",
    "      \n",
    "    Returns:\n",
    "      J (scalar): cost\n",
    "    \"\"\"\n",
    "    J = 0\n",
    "# ====================== CODE HERE ======================  \n",
    "    n = len(Y) # or 442 in this case\n",
    "    J = (1/(2*n)) * np.linalg.norm(X @ W - Y) ** 2 \n",
    "\n",
    "# ============================================================\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg_compute_gradient(X, Y, W): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      X : Data,n examples with n features\n",
    "      Y : n target values\n",
    "      W : m model parameters \n",
    "      \n",
    "    Returns:\n",
    "      dJ_dW : The gradient of the cost w.r.t. the parameters W, m values. \n",
    "    \"\"\"\n",
    "    dJ_dW = 0\n",
    "\n",
    "# ====================== CODE HERE ======================  \n",
    "\n",
    "    n = len(Y) \n",
    "    X_T = np.transpose(X) \n",
    "    dJ_dW = (1/n) * X_T  @ ((X @ W) - Y)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "    return dJ_dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def linreg_gradient_descent(X, Y, W_in, cost_function, gradient_function, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X                   : Data, n examples with m features\n",
    "      Y                   : n target values\n",
    "      W_in                : m initial model parameters  \n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      W                   : final m values of parameters \n",
    "      J (scalar)          : final cost\n",
    "      \"\"\"\n",
    "    W = 0\n",
    "    J = 0\n",
    "    \n",
    "# ====================== YOUR CODE HERE ======================  \n",
    "# DO NOT use any other import statements for this question\n",
    "    W = W_in\n",
    "    for i in range(num_iters):\n",
    "      dJ_dW = gradient_function(X, Y, W)  \n",
    "      #print(f\"Dj_Dw[{i}] : {dJ_dW}\") I wanted to verify that the values are updating\n",
    "      W = W -  alpha * dJ_dW\n",
    "      # print(f\"W[{i}] : {W}\") I wanted to verify that the values are updating\n",
    "      J = cost_function(X, Y, W) \n",
    "        \n",
    "    # Add the cost function to the array\n",
    "\n",
    "# ===========================================================\n",
    "\n",
    "    return W, J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please copy the folowing result line to Question 2 \"(sumW = )\"\n",
      "72.02\n",
      "Please copy the folowing result line to Question 3 \"(J = )\"\n",
      "1923.46\n"
     ]
    }
   ],
   "source": [
    "# initialize parameters\n",
    "initial_W = np.ones(X_train.shape[1]).reshape(5,1)\n",
    "\n",
    "# gradient descent settings\n",
    "iterations = 1000\n",
    "alpha = 0.05\n",
    "\n",
    "\"\"\"\n",
    "Apply functions coded above to calculate final W and cost J\n",
    "Use given datasets and parameters\n",
    "\"\"\"\n",
    "W = 0\n",
    "# ====================== CODE HERE ======================  \n",
    "W, J = linreg_gradient_descent(X_train, Y_train, initial_W, linreg_compute_cost, linreg_compute_gradient, alpha, iterations)\n",
    "# ============================================================\n",
    "\n",
    "print('Please copy the folowing result line to Question 2 \"(sumW = )\"')\n",
    "print(np.round(np.sum(W),2))\n",
    "print('Please copy the folowing result line to Question 3 \"(J = )\"')\n",
    "print(np.round(J,2))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
